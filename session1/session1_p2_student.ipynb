{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: part 2\n",
    "<a href=\"https://colab.research.google.com/github/ntu-dl-bootcamp/deep-learning-2024/blob/main/session1/session1_p2_student.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "\n",
    "We will apply the knowledge from previous session to analyze a dataset of houses. This dataset is obtained from the Coursera machine learning foundations course (https://www.coursera.org/learn/ml-foundations/supplement/RP8te/predicting-house-prices-assignment), a highly recommended course. It can be downloaded at https://d396qusza40orc.cloudfront.net/phoenixassets/home_data.csv.\n",
    "\n",
    "__Exercises__: Just follow along this notebook, exercises are indicated with exceptions (`raise` keyword). You are also invited to play with the code, discuss with your colleagues and ask questions to the instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data and import the libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "!rm -f home_data.csv\n",
    "!wget https://raw.githubusercontent.com/ntu-dl-bootcamp/deep-learning-2024/main/session1/home_data.csv\n",
    "# Load home_data.csv into a dataframe\n",
    "homes = pd.read_csv('home_data.csv')\n",
    "homes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 21 features and 21613 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data loaded, we can start exploring it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of price\n",
    "raise NotImplementedError(\"TODO: plot a histogram of price\")\n",
    "\n",
    "plt.___(homes['price'])\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot square feet vs price of house\n",
    "raise NotImplementedError(\"TODO: plot square feet vs price of house\")\n",
    "plt.___(homes['sqft_living'], homes['price'])\n",
    "plt.xlabel('Living Area (sqft)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Price vs Living Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot bedrooms and bathrooms vs price\n",
    "# TODO Find out how to add a legend\n",
    "raise NotImplementedError(\"TODO: add a legend in the below plot\")\n",
    "plt.scatter(homes['bedrooms'], homes['price'], ___)\n",
    "plt.scatter(homes['bathrooms'], homes['price'], ___)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Price vs Feature')\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise. generate summaries of the data for all columns except id \n",
    "# Is there a pandas API that seems helpful? \n",
    "# Check the docs at https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "raise NotImplementedError(\"TODO: generate summaries of the data for all columns except id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise. Find the most expensive house and the biggest house\n",
    "raise NotImplementedError(\"TODO: find the most expensive house and the biggest house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise. Find the average price, the median price, and the average price of a 3 bedroom house\n",
    "raise NotImplementedError(\"TODO: Find the average price, the median price, and the average price of a 3 bedroom house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use a linear regression model to predict the housing prices. As in the typical machine learning workflow, we perform the following steps:\n",
    "1. Split the data into training and test sets\n",
    "2. Train the model on the training set\n",
    "3. Evaluate the model on the test set\n",
    "\n",
    "We will use the scikit-learn library to perform the linear regression. Scikit-learn is a very popular library for machine learning in Python. It provides a wide range of machine learning algorithms and tools for data analysis and visualization. It is also very well documented, so it is a good idea to get familiar with it.\n",
    "\n",
    "For linear regression, we can use the `LinearRegression` class from scikit-learn. We will use the `fit` method to train the model and the `predict` method to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "train_homes, test_homes = train_test_split(homes, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check the size of the train and test sets\n",
    "train_homes.shape\n",
    "test_homes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build linear regression model from sqft_living to price\n",
    "\n",
    "# fit the model\n",
    "sqft_model = LinearRegression()\n",
    "sqft_model.fit(train_homes[['sqft_living']], train_homes['price'])\n",
    "\n",
    "# make predictions on the test data\n",
    "pred_homes = sqft_model.predict(test_homes[['sqft_living']])\n",
    "pred_homes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can analyze the model performance.\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients:\", sqft_model.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" %  np.sqrt(mean_squared_error(test_homes['price'], pred_homes)))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(test_homes['price'], pred_homes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions and the original data\n",
    "plt.scatter(test_homes['sqft_living'], test_homes['price'])\n",
    "plt.plot(test_homes['sqft_living'], pred_homes, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve a relatively satisfactory result. Let's see how we can improve it. Perhaps we can use more features? Try to add a few more features and compare the results. Can you improve the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"TODO: improve the model by adding more features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Discuss__: \n",
    "1. What is the difference between the training and test sets? Why do we need both?\n",
    "2. Why is the `multi_model` performing better than the original `sqft_model`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the same dataset to perform linear regression using PyTorch. We will use the same features as before, but we will use a different model. Instead of using a linear model, we will use a neural network with no hidden layer. \n",
    "\n",
    "Do not try to understand all the details of the code below. The important thing is to understand the general structure of the code and the main steps of the training process:\n",
    "1. Load the data\n",
    "2. Define the model, the loss function and the optimizer. These concepts will be introduced in more details in the next session. For now, just think of the model as a function that takes the input and returns the output. The loss function is a function that measures how good the model is. The optimizer is an algorithm that tries to minimize the loss function.\n",
    "5. Train the model for a number of epochs. Train consists of using the loss function to measure the error of the model and the optimizer to update the parameters of the model to reduce the error.\n",
    "6. Evaluate the model. Reuse the loss function to measure the error of the model on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "\n",
    "def make_training_deterministic(seed: int = 0):\n",
    "    '''Set random seed for reproducibility'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model training deterministic\n",
    "make_training_deterministic(0)\n",
    "\n",
    "# Load the data\n",
    "data = homes[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'zipcode']]\n",
    "scaler = StandardScaler()\n",
    "X = torch.tensor(scaler.fit_transform(data), dtype=torch.float32)\n",
    "y = torch.tensor(homes['price'].values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# train-test split for model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model \n",
    "model = nn.Linear(X.shape[1], y.shape[1])\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop\n",
    "n_epochs = 500 # number of epochs to run\n",
    "train_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # set model to training mode\n",
    "    # forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    # save loss for plotting\n",
    "    train_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test loop\n",
    "model.eval() # set model to evaluation mode\n",
    "y_pred = model(X_test)\n",
    "loss = loss_fn(y_pred, y_test)\n",
    "print(\"Root mean squared error: %.2f\" % np.sqrt(loss.item()))\n",
    "print(\"r2_score: %.2f\" % r2_score(y_test.detach().numpy(), y_pred.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train loss\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are able to achieve very similar results using PyTorch. It turns out that a one layer neural network is equivalent to a linear regression model. You can read more about this on https://deeplearning.neuromatch.io/tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html#bonus (see Bonus section)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
