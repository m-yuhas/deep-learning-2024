{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qmzcs5eNlV3"
      },
      "source": [
        "# NTU Deep Learning Bootcamp Hackathon Challenge\n",
        "\n",
        "## Goal\n",
        "Your goal is to classify Pokémon by primary type and secondary type given an image of the pokemon and a textual description.  The training dataset is provided to you, but you will be evaluated on a hiddent test dataset after you submit your solutions.  Highest classification accuracy on primary type is the winner.  If there is a tie, it will be broken by classification accuracy on secondary type.  If there is a tie in both of these metrics, the model with the smaller number of weights will be declared the winner.\n",
        "\n",
        "## Background\n",
        "If you are unfamiliar with Pokémon and their types, you can read more about them [here](https://bulbapedia.bulbagarden.net/wiki/Type).  Basically each Pokémon has a primary type (water, fire, electricity, etc.) and sometimes a secondary type (but not all the time).  Our claim is that given a picture of a Pokémon and its textual description, you should be able to determine both its primary and secondary types.\n",
        "\n",
        "## Rules\n",
        "* You can only use the dataset provided to you.  Hardcoding a table of Pokémon names and their corresponding types is forbidden.\n",
        "* You have until the deadline to complete the challenge.  You cannot start before, and work continued after the deadline cannot be submitted for the cash prize.\n",
        "* You are allowed to use the Internet, ChatGPT, or any other resources to help you write your code, but you must train your model yourself, i.e., you cannot take weights from somebody else online and submit them as your solution.\n",
        "* You are allowed to start with a pretrained model, including transformers.\n",
        "* You must submit both your iPython notebook and the weights (*.pt) file.  Judging will be based on the accuracy measurement function included in this notebook.  Please make sure that your model outputs guesses in the format supported by this function, or your submission will be disqualified.\n",
        "* You cannot get help from other teams enrolled in the competition.\n",
        "* You cannot get help from student studying computer science or a related field if they are not involved in the competition and enrolled on your team.\n",
        "\n",
        "## Submission\n",
        "The following items are needed for a complete submission:\n",
        "1. iPython notebook named ```hackathon_TEAMNAME.ipynb```. This notebook must contain the class for your model, as well as the transforms and tokenizer that you want us to use for testing.\n",
        "2. Your trained model weights stored in a file called ```submission_TEAMNAME.pth```.  Please save your weights as a state dictionary, do not pickle your entire model.\n",
        "\n",
        "More information is available in the *Testing and Submission* subsection below.\n",
        "\n",
        "## Getting Started\n",
        "An example solution is provided below to show you how to load the dataset and use the accuracy measurement function.  Feel free to use this as a starting point.  Lines starting with ```# HINT:``` contain advice that you may find helpful if you are unsure of how to proceed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Mvdv4TJY_4rx"
      },
      "outputs": [],
      "source": [
        "# Some basic libraries to get you started\n",
        "import numpy\n",
        "import pandas\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "from PIL import Image\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Isxnj5sL8p-",
        "outputId": "1727c760-9202-42f1-8961-7bd51a2ea89e"
      },
      "outputs": [],
      "source": [
        "# This code loads the dataset.  There is no need to modfiy this block of code.\n",
        "# https://drive.google.com/file/d/1tXKD4OXxJKvopkNpTOv2w4II4jCP_OrZ/view?usp=sharing\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown https://drive.google.com/uc?id=1tXKD4OXxJKvopkNpTOv2w4II4jCP_OrZ -O PokeData.zip\n",
        "!unzip PokeData.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdfM9SKBW1Fj"
      },
      "source": [
        "### Loading the Dataset\n",
        "In this section we load the dataset and create some sample dataloaders.  Please feel free to modify any of the blocks in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qXWveOcFDvdl"
      },
      "outputs": [],
      "source": [
        "# We provide a MultimodalPokemonDataset class for your convenience.\n",
        "# This block shows you how to load the dataset and explore its contents.\n",
        "from PokeData.pokedata import MultimodalPokemonDataset\n",
        "\n",
        "\n",
        "# You will need to define a transform to be applied to the image data in the\n",
        "# dataset.  The transform below is a simple example, but may not be optimal.\n",
        "# HINT: Torch can take care of things like data augmentation in the tranform.\n",
        "train_transform = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.ToTensor(),\n",
        "  torchvision.transforms.Resize((224, 224), antialias=True),\n",
        "])\n",
        "\n",
        "\n",
        "# You will also need to define a tokenizer for the text data.  The max_length\n",
        "# argument gives the maximum number of tokens returned for an input sequence.\n",
        "# By default, sequnces shorter than max_length are padded.\n",
        "# HINT: If you load a pretrained text model, make sure it was trained on data\n",
        "# from the same tokenizer you load here.\n",
        "train_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "# Load our dataset\n",
        "dataset = MultimodalPokemonDataset(\n",
        "  root='PokeData',\n",
        "  transform=train_transform,\n",
        "  tokenizer=train_tokenizer,\n",
        "  max_length=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Rd3HGQtCYr",
        "outputId": "790244ab-39a5-42f3-e56c-13f83f45b9b0"
      },
      "outputs": [],
      "source": [
        "# You can access the list of images like this:\n",
        "dataset.img_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "p6LXZf1-Vhzn",
        "outputId": "3caa978f-59d1-4451-f0d4-79ff366b7e1b"
      },
      "outputs": [],
      "source": [
        "# To view an untransformed image:\n",
        "img = Image.open(dataset.img_list[0])\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13kLH9WGV1Qa",
        "outputId": "e07bb1d6-57f1-4a5d-f136-94bdc4cb3eb6"
      },
      "outputs": [],
      "source": [
        "# To view the raw text labels:\n",
        "dataset.descr_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuDmxzVCV_GV",
        "outputId": "ebda7bb1-3a6e-4467-f3aa-1abb3fd26e3b"
      },
      "outputs": [],
      "source": [
        "# To view the type data:\n",
        "print(f'Primary Type: {dataset.primary_type_list[:10]}')\n",
        "print(f'Secondary Type: {dataset.secondary_type_list[:10]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nDkuiPdKWK8J",
        "outputId": "1eddeebd-eb90-4bec-cc72-4ac65c9ac8ed"
      },
      "outputs": [],
      "source": [
        "# To convert the number to a type\n",
        "dataset.index_to_type(dataset.primary_type_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT5EjCIzFK1Y",
        "outputId": "7d57aac2-c73e-4924-bf9b-68184351f0b7"
      },
      "outputs": [],
      "source": [
        "# In total we have 19 possible types for Pokemon (including NA when there is no\n",
        "# secondary type)\n",
        "NUM_TYPES = len(dataset.type_to_index)\n",
        "print(\"All possible Pokemon types:\")\n",
        "print(dataset.type_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GZ3Jfxh2XHwR"
      },
      "outputs": [],
      "source": [
        "# Let's split the dataset and create data loaders.\n",
        "# HINT: Right now the test/val split is set at 90/10, but this may not be\n",
        "# optimal.\n",
        "# HINT: This dataset is highly imbalanced.  You may want to oversample or\n",
        "# undersample some classes or use K-fold cross-validation.\n",
        "train_set, val_set = torch.utils.data.random_split(\n",
        "  dataset,\n",
        "  [int(0.9 * len(dataset)), int(0.1 * len(dataset)) + 1]\n",
        ")\n",
        "\n",
        "\n",
        "# HINT: Remember that batch size is a hyperparemter you can adjust.\n",
        "BATCH_SIZE = 16\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fssOSYE_Yf1C"
      },
      "source": [
        "### Create the Model\n",
        "Below is a simple example of a multimodal model.  It uses a separate backbone for text and image data and combines them in a final linear layer.  You can use this as a starting point, or try something completely difference.  Remember, nothing says you necessarily have to use both data modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "G26IElBfxpxV"
      },
      "outputs": [],
      "source": [
        "class PokeClassifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    # HINT: Here we are using pretrained BERT as a text backbone, but other\n",
        "    # language models with varying levels of performance may be available.\n",
        "    super().__init__()\n",
        "    self.text_backbone = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "    text_backbone_outsize = self.text_backbone.config.hidden_size\n",
        "    # HINT: Below we freeze the layers of pretrained BERT before fine tuning\n",
        "    # to save time, but if you are using a simpler language model, you may want\n",
        "    # to investigate updating the backbone weights as well.\n",
        "    for param in self.text_backbone.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # HINT: Here we are using pretrained Resnet50 as a vision backbone, but\n",
        "    # other image encoders are available.\n",
        "    self.vision_backbone = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "    vision_backbone_outsize = self.vision_backbone.fc.in_features\n",
        "    self.vision_backbone.fc = torch.nn.Identity()\n",
        "    # HINT: Below we freeze the layers of the pretrained Resnet before fine\n",
        "    # tuning, but you may want to experiment with updating the backbone weights\n",
        "    # as well.\n",
        "    for param in self.vision_backbone.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # HINT: Here we are just taking all the outputs of BERT and all the outputs\n",
        "    # of Resnet, shmooshing them together and feeding them into a linear layer\n",
        "    # for classification (actually, two linear layers running in parallel, one\n",
        "    # for primary type and the other for secondary type).  This is probably the\n",
        "    # simplest way to combine data of different modalities, but it may lake the\n",
        "    # capacity to learn more complex relationships.  Can you find a better one?\n",
        "    self.fc_primary_type = torch.nn.Linear(\n",
        "      text_backbone_outsize + vision_backbone_outsize,\n",
        "      NUM_TYPES,\n",
        "    )\n",
        "    self.fc_secondary_type = torch.nn.Linear(\n",
        "      text_backbone_outsize + vision_backbone_outsize,\n",
        "      NUM_TYPES,\n",
        "    )\n",
        "\n",
        "  def forward(self, img, txt, attn):\n",
        "    # Inference the text backbone\n",
        "    bertput = self.text_backbone(\n",
        "      input_ids=txt.squeeze(1),\n",
        "      attention_mask=attn.squeeze(1)\n",
        "    )\n",
        "    text_features = bertput.pooler_output\n",
        "\n",
        "    # Inference the vision backbone\n",
        "    vision_features = self.vision_backbone(img)\n",
        "\n",
        "    # Combine the feature spaces and run our classifiers\n",
        "    feature_space = torch.cat((text_features, vision_features), dim=-1)\n",
        "    primary_type = self.fc_primary_type(feature_space)\n",
        "    secondary_type = self.fc_secondary_type(feature_space)\n",
        "    return primary_type, secondary_type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjkWWbFTfP_f"
      },
      "source": [
        "### Training the Model\n",
        "Below is some sample training code for the model.  Remember, the training process (loss function, learning rate, optimizer, etc.) have just as big of an impact on model performance as architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lf_Ngve_2Zr",
        "outputId": "f8d473d1-af60-4f77-fd3e-12b4c57be7d1"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our classifier\n",
        "model = PokeClassifier()\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "\n",
        "# Optional: load from a previous checkpoint\n",
        "#model.load_state_dict(torch.load('checkpoint.pth'))\n",
        "\n",
        "\n",
        "# HINT: Adam may not be the optimal optimzer or the hyperparameters passed here\n",
        "# may not be the best.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# HINT: Here we are using cross entropy loss, but other more complex loss\n",
        "# functions are possible.\n",
        "primary_loss = torch.nn.CrossEntropyLoss()\n",
        "secondary_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# HINT: Remember, training for more epochs can improve loss, but at some point\n",
        "# it will lead to overfitting.\n",
        "N_EPOCHS = 10\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  # Training Loop\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  for batch in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get data from the data loader and push it to the GPU\n",
        "    img, txt, attn, primary_gt, secondary_gt = batch\n",
        "    img = img.to(DEVICE)\n",
        "    txt = txt.to(DEVICE)\n",
        "    attn = attn.to(DEVICE)\n",
        "    primary_gt = primary_gt.to(DEVICE)\n",
        "    secondary_gt = secondary_gt.to(DEVICE)\n",
        "\n",
        "    # Inference the model\n",
        "    primary_guess, secondary_guess = model(img, txt, attn)\n",
        "\n",
        "    # Calculate loss and update weights\n",
        "    # HINT: Here we are weighting both losses equally, but it may be more\n",
        "    # beneficial to focus on one loss over the other.  Also, there is nothing\n",
        "    # that says you can't use two separate optimizers for two separate losses.\n",
        "    L = primary_loss(primary_guess, primary_gt) + secondary_loss(secondary_guess, secondary_gt)\n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += L\n",
        "\n",
        "  # Validation Loop\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  for batch in val_loader:\n",
        "    # Get data from the data loader and push it to the GPU\n",
        "    img, txt, attn, primary_gt, secondary_gt = batch\n",
        "    img = img.to(DEVICE)\n",
        "    txt = txt.to(DEVICE)\n",
        "    attn = attn.to(DEVICE)\n",
        "    primary_gt = primary_gt.to(DEVICE)\n",
        "    secondary_gt = secondary_gt.to(DEVICE)\n",
        "\n",
        "    # Inference the model and calculate loss\n",
        "    with torch.no_grad():\n",
        "      primary_guess, secondary_guess = model(img, txt, attn)\n",
        "      L = primary_loss(primary_guess, primary_gt) + secondary_loss(secondary_guess, secondary_gt)\n",
        "      val_loss += L\n",
        "\n",
        "  # Report results\n",
        "  print(f'Epoch: {epoch} '\n",
        "        f'-- Training Loss: {train_loss / len(train_set):.3f} '\n",
        "        f'-- Val Loss: {val_loss / len(val_set):.3f} '\n",
        "  )\n",
        "\n",
        "  # Save a checkpoint\n",
        "  torch.save(model.state_dict(), 'checkpoint.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNvjwffIiJ6Y"
      },
      "source": [
        "### Testing and Submission\n",
        "A sample test function is provided in this section.  Note that all you are required to supply is a model, an input transform, tokenizer, and max length for a tokenized string.  Here, we load the dataset provided to you, but in your final submission, you will be evaluated against a blind test set.\n",
        "\n",
        "Also, don't forget to save your model weights and download them **before logging off your VM**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr3Zq4BMy5ft",
        "outputId": "da080455-5bb0-473c-b022-2b2da261e111"
      },
      "outputs": [],
      "source": [
        "def measure_accuracy(model, transform, tokenizer, max_length):\n",
        "  # Create a data loader specifically for testing\n",
        "  test_set = MultimodalPokemonDataset(\n",
        "    root='PokeData',\n",
        "    transform=train_transform,\n",
        "    tokenizer=train_tokenizer,\n",
        "    max_length=max_length,\n",
        "  )\n",
        "  test_loader = torch.utils.data.DataLoader(test_set)\n",
        "\n",
        "  # Evaluate the model\n",
        "  model.eval()\n",
        "  n_correct_pri = 0\n",
        "  n_correct_sec = 0\n",
        "  n_samples = 0\n",
        "  for batch in test_loader:\n",
        "    # Get data from the data loader and push it to the GPU\n",
        "    img, txt, attn, pri_gt, sec_gt = batch\n",
        "    img = img.to(DEVICE)\n",
        "    txt = txt.to(DEVICE)\n",
        "    attn = attn.to(DEVICE)\n",
        "    pri_gt = pri_gt.to(DEVICE)\n",
        "    sec_gt = sec_gt.to(DEVICE)\n",
        "\n",
        "    # Inference the model and accuracy\n",
        "    with torch.no_grad():\n",
        "      pri_type_guess, sec_type_guess = model(img, txt, attn)\n",
        "      n_correct_pri += torch.sum(torch.argmax(pri_type_guess, 1) == pri_gt)\n",
        "      n_correct_sec += torch.sum(torch.argmax(sec_type_guess, 1) == sec_gt)\n",
        "      n_samples += pri_gt.shape[-1]\n",
        "\n",
        "  # Display the results\n",
        "  print(f'Accuracy on Primary Type: {100 * n_correct_pri / n_samples:.3f}%')\n",
        "  print(f'Accuracy on Secondary Type: {100 * n_correct_sec / n_samples:.3f}%')\n",
        "\n",
        "\n",
        "# Please remember to update this line if you use different variable names or\n",
        "# a separate transform for testing.\n",
        "measure_accuracy(model, train_transform, train_tokenizer, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aJbtE3BqF2E1"
      },
      "outputs": [],
      "source": [
        "# You can save a model with the command below\n",
        "torch.save(model.state_dict(), 'submission.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
