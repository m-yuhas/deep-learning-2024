{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "### Agenda\n",
        "- Previous architectures: RNNs, LSTMs\n",
        "- Current architectures: Transformers\n",
        "- Current tools: LLMs\n",
        "- Fine-tuning LLMs\n"
      ],
      "metadata": {
        "id": "8N18I7oMYnpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ntu-dl-bootcamp/deep-learning-2024/blob/main/session4/session4_student.ipynb)"
      ],
      "metadata": {
        "id": "iVfE9L7Xbces"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNNs and LSTMs\n",
        "\n",
        "Previously RNNs were a popular method used to process text. They were theoretically able to handle long-range dependencies well, but they were computationally inefficient. They frequently ran into vanishing / exploding gradients issues. They also processed sequentially, and hence could not be parallelized across GPUs.\n",
        "> *Pop Quiz:* What is the vanishing gradients problem in DNNs? [Explanation](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/)    \n",
        "Why do you think it becomes a huge problem in RNNs?\n",
        "\n",
        "__Long range problem :__ \"Lionel Andrés \"Leo\" Messi was born in Argentina in 24 June 1987 and is a professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the national team. Messi speaks fluent <?>.”\n",
        "\n",
        "__Unidirectional problem :__ \"Mouse is really good. The mouse is used to <?> for the easy use of computers.”\n",
        "\n",
        "The \"attention\" mechanism came about as a way to handle sequence length dependencies more efficiently.\n",
        "- It can be parallelized, allowing us to use GPUs for training.\n",
        "- It is computationally efficient, allowing us to process much larger models on larger datasets\n",
        "- Can be used for classification, sequence generation, sequence labeling, translation etc.\n"
      ],
      "metadata": {
        "id": "2PvzPwwAhbx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT\n",
        "BERT stands for __B__idirectional __E__ncoder __R__epresentations from __T__ransformers. It is a way to create *representations* of text to be used with Transformer models.\n",
        "\n",
        "Q: Is BERT a transformer?\n",
        "\n",
        "A: BERT is a transformer-*based* model. It uses an encoder that is very similar to the original encoder of the transformer.\n",
        "\n",
        "\n",
        "Q: Is BERT same as GPT?\n",
        "\n",
        "A: BERT uses the encoder segment whereas GPT uses the decoder segment of a transformer.\n",
        "\n",
        "\n",
        "BERT understands language by training for 2 NLP tasks *together*-\n",
        "- Masked Language Model - \"The [MASK1] brown fox [MASK2] over the lazy dog.\"\n",
        "  \n",
        "  BERT fills in the blanks.\n",
        "- Next Sentence Prediction - \"A: Harry Potter is a wizard.\"  B: He goes to the Hogwarts School of Witchcraft and Wizardry.\"   \n",
        "BERT tries to answer if sentence B follows A.\n",
        "\n"
      ],
      "metadata": {
        "id": "tu2PTjjin2Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embeddings\n",
        "\n",
        "Embeddings are a way to represent words as numbers. We want computers to not just read/write the words but also understand its meaning and context. A good word embedding model must cluster similar words together and establish a relationship between them.   \n",
        "\n",
        "\n",
        "### One Hot Encoding\n",
        "\n",
        "Corpus = \"The quick brown fox jumps over the lazy dog\"    \n",
        "Vocabulary = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\"]   \n",
        "OHE vector for \"brown\" = [0 0 1 0 0 0 0 0]   \n"
      ],
      "metadata": {
        "id": "iKgHzGHclaay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec\n",
        "Preserves word meaning and outputs a vector for each word.\n",
        "\n",
        "Word vector for \"brown\" = [0.0012 0.1091 0.7386 0.0105 0.0585 0.0084 0.0057 0.0063]   \n",
        "\n",
        "There is no *meaning* to these numbers, in terms of nouns, verbs etc."
      ],
      "metadata": {
        "id": "_6p8_rhbr55a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for word2Vec using Gensim.\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "\n",
        "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
        "\n",
        "# Example to find nearby words.\n",
        "\n",
        "print(\"3 similar words to beautiful:\")\n",
        "words = google_news_vectors.most_similar(\"beautiful\", topn=3)\n",
        "for word in words:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "cjOAe6jRiquI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Capital of Britain given Capital of France: (Paris - France) + Britain =\n",
        "\n",
        "print(\"Finding Capital of Britain: (Paris - France) + Britain\")\n",
        "capital = google_news_vectors.most_similar([\"Paris\", \"Britain\"], [\"France\"], topn=1)\n",
        "print(capital)"
      ],
      "metadata": {
        "id": "v9Nbyyi2wF5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example to measure similarity.\n",
        "\n",
        "cosine = google_news_vectors.similarity(\"king\", \"queen\")\n",
        "print(\"Cosine similarity between king and queen:\", cosine)\n",
        "print()\n",
        "\n",
        "cosine = google_news_vectors.similarity(\"king\", \"rock\")\n",
        "print(\"Cosine similarity between king and rock:\", cosine)"
      ],
      "metadata": {
        "id": "0Lbl7IFJwOrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: Write your own Word2Vec model"
      ],
      "metadata": {
        "id": "-NUjievuwvmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "nltk.download(\"brown\") # Explore other corpora here - https://www.nltk.org/nltk_data/\n",
        "\n",
        "# Preprocessing data to lowercase all words and remove single punctuation words\n",
        "document = brown.sents()\n",
        "data = []\n",
        "for sent in document:\n",
        "  new_sent = []\n",
        "  for word in sent:\n",
        "    new_word = ... # Convert word to lower case\n",
        "    if new_word[0] not in string.punctuation:\n",
        "      ... # Append to new_sent\n",
        "  if len(new_sent) > 0:\n",
        "    ... # Append to data\n",
        "\n",
        "# Creating Word2Vec\n",
        "model = Word2Vec(\n",
        "    sentences = ..., # Your cleaned corpus\n",
        "    vector_size = 50,\n",
        "    window = 10,\n",
        "    epochs = 20,\n",
        ")\n",
        "\n",
        "# Vector for word\n",
        "print(\"Vector for technology:\")\n",
        "print(model.wv[\"technology\"])"
      ],
      "metadata": {
        "id": "ipIPgncpwz09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding most similar words\n",
        "print(\"3 words similar to car\")\n",
        "words = ...\n",
        "for word in words:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "HrEdAG7sxS0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing data\n",
        "words = [\"france\", \"india\", \"truck\", \"boat\", \"road\", \"teacher\", \"student\"]\n",
        "\n",
        "X = model.wv[words]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "wT5xC-FvxFk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT embeddings\n"
      ],
      "metadata": {
        "id": "xyznqMdpymN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer([\"hello, world!\"])\n",
        "print(tokens['input_ids'])"
      ],
      "metadata": {
        "id": "YdEo0iDozsWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is a pretrained model that expects input data in a specific format.\n",
        "\n",
        "- A special token, [SEP], to mark the end of a sentence, or the separation between two sentences\n",
        "- A special token, [CLS], at the beginning of our text. This token is used for classification tasks, but *BERT expects it no matter what your application* is\n",
        "- Keep sentences the same length using truncation or padding tokens [PAD]\n",
        "- Mask IDs [MASK] to indicate which elements in the sequence are tokens and which are padding elements. When you add padding tokens to make sentences the same length, you also use an “attention mask.” This is like a map that helps BERT know which parts are actual words (marked as 1) and which are padding (marked as 0).\n",
        "- Segment IDs used to distinguish different sentences\n",
        "- Positional Embeddings used to show token position within the sequence\n"
      ],
      "metadata": {
        "id": "AVh2D1qp1OQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize the sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "metadata": {
        "id": "NAfziNdo1zIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original word \"embeddings\" has been split into smaller subwords and characters. The two hash signs preceding some of these subwords are just the tokenizer's way to denote that this subword or character is part of a larger word and preceded by another subword. i.e., the '##bed' token is separate from the 'bed' token.\n",
        "\n",
        "Why does it look this way?    \n",
        "This is because the BERT tokenizer was created with a WordPiece model which has a fixed-size vocabulary of 30,000. So it tries to be efficient by reusing common prefixes and postfixes."
      ],
      "metadata": {
        "id": "9NyOJ1Mk1qtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the token strings to their vocabulary indices.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indices.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "metadata": {
        "id": "FW_aQ8On2gAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT Embeddings"
      ],
      "metadata": {
        "id": "DTv3TY-OF-aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "context = tokenizer('It will rain in the', return_tensors='pt')\n",
        "\n",
        "prediction = gpt2.generate(**context, max_length=10)\n",
        "tokenizer.decode(prediction[0])"
      ],
      "metadata": {
        "id": "D9fMsdBeCkkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "sentences = [\"It will rain in the\",\n",
        "            \"I want to eat a big bowl of\",\n",
        "            \"My dog is\"]\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "fPo6LSlOGJkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_sequences = gpt2.generate(**inputs)\n",
        "\n",
        "for seq in output_sequences:\n",
        "    print(tokenizer.decode(seq))"
      ],
      "metadata": {
        "id": "-ifMEjNcGg6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipelines\n",
        "Hugging Face provides [pipelines](https://huggingface.co/docs/transformers/en/main_classes/pipelines) which make it easier for you to perform end-to-end tasks like machine translation, sentiment analysis, q&a etc. Pipelines mainly consist of a tokenizer and a task-specific model."
      ],
      "metadata": {
        "id": "CNWfnkukjXpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for using a Hugging Face pipeline to perform classification.\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "pipecl = pipeline(\"text-classification\")\n",
        "pipecl(\"This restaurant is awesome\")"
      ],
      "metadata": {
        "id": "aBJdPQYF4dqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for using a Hugging Face pipeline to perform machine translation.\n",
        "\n",
        "translator = pipeline(\"translation_en_to_fr\")\n",
        "translation = translator(\"What's your name?\")\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "LqKP5l7birYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a summarizer using pipelines. Refer available Hugging Face pipelines [here](https://huggingface.co/docs/transformers/en/main_classes/pipelines)"
      ],
      "metadata": {
        "id": "LgROellaZb78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a summarizer\n",
        "\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "!wget -nc https://www.dropbox.com/s/7hb8bwbtjmxovlc/bbc_text_cls.csv?dl=0\n",
        "df = pd.read_csv('bbc_text_cls.csv?dl=0')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "k3nBmtVw-XTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = df[df.labels == 'business']['text'].sample(random_state=123)\n",
        "\n",
        "def wrap(x):\n",
        "  return textwrap.fill(x, replace_whitespace = False, fix_sentence_endings = True)\n",
        "print(\"Original: \")\n",
        "print(wrap(doc.iloc[0]))\n",
        "\n",
        "summarizer = ... # Use summarization pipeline\n",
        "print(\"Summary: \")\n",
        "summarizer(doc.iloc[0].split('\\n',1)[1])"
      ],
      "metadata": {
        "id": "FbB_tbnp_5kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Transformers\n",
        "\n",
        "> Pop Quiz : Why pre-train a transformer?\n",
        "\n",
        "> Pop Quiz : Why fine-tune a transformer?\n",
        "\n",
        "NOTE: Fine-tuning BERT is done by adding a task-specific layer on top of BERT model and training. Whereas, fine-tuning GPT is done via task-specific prompts to adapt the model."
      ],
      "metadata": {
        "id": "LMZcXtmEmNYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -U accelerate\n",
        "# ! pip install -U transformers\n",
        "# ! pip install transformers datasets\n",
        "# !pip install evaluate"
      ],
      "metadata": {
        "id": "mlnmq4BZMes4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "dataset[\"train\"][100]"
      ],
      "metadata": {
        "id": "VgMGW_Q9Ckwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "eR1LPj28Ck4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
      ],
      "metadata": {
        "id": "_m9ABwLpH26d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pretrained head of the BERT model is discarded, and replaced with a randomly initialized classification head. You will fine-tune this new model head on your sequence classification task."
      ],
      "metadata": {
        "id": "sHaDpkcPIBHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your evaluation metrics\n",
        "\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  logging_steps = 3,\n",
        "                                  num_train_epochs=3,\n",
        "                                  load_best_model_at_end=True)\n",
        "\n",
        "# Define your Trainer object\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Let the fine-tuning begin\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "hoxN0WBcIGAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(small_eval_dataset)\n",
        "prediction_class = list(np.argmax(predictions.predictions, axis=-1))\n",
        "\n",
        "print(wrap(small_eval_dataset[5]['text']))\n",
        "print(\"Original label =\",small_eval_dataset[5]['label'],\"  Predicted label =\", prediction_class[5])"
      ],
      "metadata": {
        "id": "kQsxn10cIxpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('final_model')\n",
        "tokenizer.save_pretrained('final_model')\n",
        "\n",
        "# mymodel = AutoModelForSequenceClassification.from_pretrained(\"./final_model/\")"
      ],
      "metadata": {
        "id": "GX0ZPze1RyCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = ... # create pipeline using your local model\n",
        "classifier(small_eval_dataset[5]['text'])"
      ],
      "metadata": {
        "id": "orkax6bOUwQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eJJHqXRDXE1p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}